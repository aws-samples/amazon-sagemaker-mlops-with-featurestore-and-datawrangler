{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Context\n",
    "\n",
    "This notebook demonstrate a sample of the activities and files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import requests\n",
    "import sagemaker\n",
    "import string\n",
    "\n",
    "from pathlib import Path\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case\n",
    "\n",
    "In this example, we will use an auto insurance domain to detect claims that are possibly fraudulent.\n",
    "more precisley we address the use-case: “what is the likelihood that a given autoclaim is fraudulent?,” and explore the technical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "boto_session = sagemaker_session.boto_session\n",
    "sagemaker_client = boto_session.client('sagemaker')\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "s3_uploader = sagemaker.s3.S3Uploader\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"mlops-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The inputs for building our model and workflow are two tables of insurance data: a claims table and a customers table. This data was synthetically generated is provided to you in its raw state for pre-processing with SageMaker Data Wrangler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://github.com/aws/amazon-sagemaker-examples/raw/master/end_to_end/fraud_detection/data/\"\n",
    "file_list = [\"claims.csv\", \"customers.csv\"]\n",
    "feature_eng_base_path = Path(\"feature_engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = Path(\"data\")\n",
    "local_path.mkdir(exist_ok=True)\n",
    "for file_url in file_list:\n",
    "    file_url = base_url + file_url\n",
    "    parsed_url = parse.urlparse(file_url)\n",
    "    file_name = Path(parsed_url.path).name\n",
    "    file_path = local_path / file_name\n",
    "    with file_path.open(\"wb\") as f, requests.get(file_url, stream=True) as r:\n",
    "        for chunk in r.iter_content():\n",
    "            f.write(chunk)\n",
    "    logger.info(f\"Retrieved {file_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the files to S3, and open it on SageMaker Data Wrangler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uri_prefix = s3_uploader.upload(local_path.as_posix(), f\"s3://{bucket}/{prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_uri = data_uri_prefix + \"/claims.csv\"\n",
    "customers_uri = data_uri_prefix + \"/customers.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing the tempalte `flow` files to point at the correct dataset in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "claims_flow_template_file = feature_eng_base_path / \"claims_flow_template\"\n",
    "\n",
    "with (feature_eng_base_path / \"claims_flow_template\").open(\"r\") as f, (feature_eng_base_path / \"claims.flow\").open(\"w\") as g:\n",
    "    variables = {\"data_uri\": (data_uri_prefix + \"/claims.csv\")}\n",
    "    template = string.Template(f.read())\n",
    "    claims_flow = template.substitute(variables)\n",
    "    claims_flow = json.loads(claims_flow)\n",
    "    json.dump(claims_flow, g, indent=2)\n",
    "    logger.info(\"Created claims.flow \")\n",
    "\n",
    "with (feature_eng_base_path / \"customers_flow_template\").open(\"r\") as f, (feature_eng_base_path / \"customers.flow\").open(\"w\") as g:\n",
    "    variables = {\"data_uri\": (data_uri_prefix + \"/customers.csv\")}\n",
    "    template = string.Template(f.read())\n",
    "    claims_flow = template.substitute(variables)\n",
    "    claims_flow = json.loads(claims_flow)\n",
    "    json.dump(claims_flow, g, indent=2)\n",
    "    logger.info(\"Created customers.flow \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review the feature engineering:\n",
    "- Let's look at the feature engineering for the [Claims Dataset](claims.flow)\n",
    "\n",
    "- Let's look at the feature engineering for the [Customers Dataset](customers.flow)\n",
    "\n",
    "For each flow file we generate the corresponding Jupyter Notebook for export to Feature Store, to extract the data schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "from utils.feature_store_utils import get_fg_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(\n",
    "    service_name='sagemaker-featurestore-runtime',\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_offline_s3_uri = f\"s3://{bucket}/{prefix}/fs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claims Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_fg_conf = get_fg_conf(feature_eng_base_path / 'claims.fg.json', feature_store_offline_s3_uri)\n",
    "feature_definitions = claims_fg_conf['feature_definitions']\n",
    "feature_group_name = name_from_base(claims_fg_conf['feature_group_name'])\n",
    "feature_group_properties = claims_fg_conf['feature_group_properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_feature_group = FeatureGroup(\n",
    "    name=feature_group_name,\n",
    "    sagemaker_session=feature_store_session,\n",
    "    feature_definitions=feature_definitions\n",
    ")\n",
    "\n",
    "claims_feature_group.create(\n",
    "    role_arn=role,\n",
    "    **feature_group_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customers Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_fg_conf = get_fg_conf(\n",
    "    feature_eng_base_path / 'customers.fg.json',\n",
    "    feature_store_offline_s3_uri\n",
    ")\n",
    "feature_definitions = customers_fg_conf['feature_definitions']\n",
    "feature_group_name = name_from_base(customers_fg_conf['feature_group_name'])\n",
    "feature_group_properties = customers_fg_conf['feature_group_properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_feature_group = FeatureGroup(\n",
    "    name=feature_group_name,\n",
    "    sagemaker_session=feature_store_session,\n",
    "    feature_definitions=feature_definitions)\n",
    "\n",
    "customers_feature_group.create(\n",
    "    role_arn=role,\n",
    "    **feature_group_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import (\n",
    "    FeatureStoreOutput,\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    ")\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.wrangler.processing import DataWranglerProcessor\n",
    "\n",
    "from utils.parse_flow import FlowFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_fi(\n",
    "    role: str,\n",
    "    pipeline_name: str,\n",
    "    sagemaker_session:sagemaker.Session()=None,\n",
    "    **kwarg\n",
    ")-> Pipeline:\n",
    "    \"\"\"Create a SageMaker pipeline\n",
    "\n",
    "    Args:\n",
    "        role (str): IAM role that executes the pipeline\n",
    "        pipeline_name (str): name of the pipeline\n",
    "        sagemaker_session ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: Sagemaker pipeline\n",
    "    \"\"\"\n",
    "    flow_file_path = kwarg[\"flow_file_path\"]\n",
    "    feature_group_name = kwarg[\"feature_group_name\"]\n",
    "\n",
    "    flow_file = FlowFile(flow_file_path)\n",
    "\n",
    "    instance_count = ParameterInteger(name=\"InstanceCount\", default_value=1)\n",
    "    instance_type = ParameterString(name=\"InstanceType\", default_value=\"ml.m5.4xlarge\")\n",
    "    input_data_uri = ParameterString(name=\"InputDataUri\")\n",
    "\n",
    "    output_content_type = \"CSV\"\n",
    "    output_config = {flow_file.output_name: {\"content_type\": output_content_type}}\n",
    "    job_argument = [f\"--output-config '{json.dumps(output_config)}'\"]\n",
    "\n",
    "    data_sources = [\n",
    "        ProcessingInput(\n",
    "            input_name=flow_file.input_name,\n",
    "            source=input_data_uri,\n",
    "            destination=f\"/opt/ml/processing/{flow_file.input_name}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    outputs = [\n",
    "        ProcessingOutput(\n",
    "            output_name=flow_file.output_name,\n",
    "            app_managed=True,\n",
    "            feature_store_output=FeatureStoreOutput(\n",
    "                feature_group_name=feature_group_name\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    data_wrangler_processor = DataWranglerProcessor(\n",
    "        role=role,\n",
    "        data_wrangler_flow_source=flow_file_path,\n",
    "        instance_count=instance_count,\n",
    "        instance_type=instance_type,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "    data_wrangler_step = ProcessingStep(\n",
    "        name=\"data-wrangler-step\",\n",
    "        processor=data_wrangler_processor,\n",
    "        inputs=data_sources,\n",
    "        outputs=outputs,\n",
    "        job_arguments=job_argument,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[instance_count, instance_type, input_data_uri],\n",
    "        steps=[data_wrangler_step],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claims feature ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_pipeline_conf = {\n",
    "    \"flow_file_path\":'claims.flow',\n",
    "    \"feature_group_name\": claims_feature_group.name,\n",
    "}\n",
    "claims_pipeline = create_pipeline_fi(\n",
    "    role,\n",
    "    \"claims-pipeline\",\n",
    "    sagemaker_session,\n",
    "    **claims_pipeline_conf\n",
    ")\n",
    "claims_pipeline.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_pipeline.create(\n",
    "    role_arn=role,\n",
    "    description=\"Claims feature ingestion pipeline\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_pipeline_execution = claims_pipeline.start(\n",
    "    parameters={\n",
    "        \"InputDataUri\": claims_uri\n",
    "    },\n",
    "    execution_display_name=\"initial-load\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_pipeline_execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customers feature ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_pipeline_conf = {\n",
    "    \"flow_file_path\":'customers.flow',\n",
    "    \"feature_group_name\": customers_feature_group.name,\n",
    "}\n",
    "customers_pipeline = create_pipeline_fi(\n",
    "    role,\n",
    "    \"customers-pipeline\",\n",
    "    sagemaker_session,\n",
    "    **customers_pipeline_conf\n",
    ")\n",
    "customers_pipeline.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_pipeline.create(\n",
    "    role_arn=role,\n",
    "    description=\"Customers feature ingestion pipeline\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_pipeline_execution = customers_pipeline.start(\n",
    "    parameters={\n",
    "        \"InputDataUri\": customers_uri\n",
    "    },\n",
    "    execution_display_name=\"initial-load\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Here we define a SageMaker Pipeline to manage all the tasks necesary for model training:\n",
    "- data extraction from Feature Store\n",
    "- data processing specific to Model Training, e.g., joining datasets, traint/test split\n",
    "- train the model\n",
    "- evaluate initial dataset bias across features\n",
    "- record a basline of the distribution of the training dataset for Model Monitor use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.dataset_definition.inputs import (\n",
    "    DatasetDefinition,\n",
    "    AthenaDatasetDefinition,\n",
    ")\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    Processor,\n",
    ")\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep, TrainingStep\n",
    "from sagemaker import clarify\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_xgboost_training(\n",
    "    role: str, pipeline_name: str, sagemaker_session: sagemaker.Session = None, **kwargs\n",
    ") -> Pipeline:\n",
    "\n",
    "\n",
    "    customers_fg_name = kwargs[\"customers_fg_name\"]\n",
    "    claims_fg_name = kwargs[\"claims_fg_name\"]\n",
    "    create_dataset_script_path = kwargs[\"create_dataset_script_path\"]\n",
    "    prefix = kwargs[\"prefix\"]\n",
    "    model_entry_point = kwargs[\"model_entry_point\"]\n",
    "    model_package_group_name = kwargs[\"model_package_group_name\"]\n",
    "    \n",
    "    customer_fg = sagemaker_client.describe_feature_group(FeatureGroupName=customers_fg_name)\n",
    "    claims_fg = sagemaker_client.describe_feature_group(FeatureGroupName=claims_fg_name)\n",
    "    database_name = customer_fg['OfflineStoreConfig']['DataCatalogConfig']['Database']\n",
    "    claims_table = claims_fg['OfflineStoreConfig']['DataCatalogConfig']['TableName']\n",
    "    customers_table = customer_fg['OfflineStoreConfig']['DataCatalogConfig']['TableName']\n",
    "    catalog = customer_fg['OfflineStoreConfig']['DataCatalogConfig']['Catalog']\n",
    "\n",
    "    train_instance_param = ParameterString(\n",
    "        name=\"TrainingInstance\",\n",
    "        default_value=\"ml.m4.xlarge\",\n",
    "    )\n",
    "\n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    "    )\n",
    "    baseline_instance_type = ParameterString(\n",
    "        name=\"BaselineInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    "    )\n",
    "\n",
    "    # Create dataset step\n",
    "    create_dataset_processor = SKLearnProcessor(\n",
    "        framework_version=\"0.23-1\",\n",
    "        role=role,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        instance_count=1,\n",
    "        base_job_name=f\"{prefix}/fraud-demo-create-dataset\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "    training_columns_string = \", \".join(f'\"{c}\"' for c in training_columns)\n",
    "\n",
    "    query_string = f\"\"\"\n",
    "    SELECT DISTINCT {training_columns_string}\n",
    "    FROM \"{claims_table}\" claims LEFT JOIN \"{customers_table}\" customers\n",
    "    ON claims.policy_id = customers.policy_id\n",
    "    \"\"\"\n",
    "    athena_data_path = \"/opt/ml/processing/athena\"\n",
    "    \n",
    "    data_sources = [\n",
    "        ProcessingInput(\n",
    "            input_name=\"athena_dataset\",\n",
    "            dataset_definition=DatasetDefinition(\n",
    "                local_path=athena_data_path,\n",
    "                data_distribution_type=\"FullyReplicated\",\n",
    "                athena_dataset_definition=AthenaDatasetDefinition(\n",
    "                    catalog=catalog,\n",
    "                    database=database_name,\n",
    "                    query_string=query_string,\n",
    "                    output_s3_uri=f\"s3://{bucket}/{prefix}/athena/data/\",\n",
    "                    output_format=\"PARQUET\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    create_dataset_step = ProcessingStep(\n",
    "        name=\"CreateDataset\",\n",
    "        processor=create_dataset_processor,\n",
    "        inputs=data_sources,\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train_data\",\n",
    "                source=\"/opt/ml/processing/output/train\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test_data\",\n",
    "                source=\"/opt/ml/processing/output/test\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"baseline\",\n",
    "                source=\"/opt/ml/processing/output/baseline\"\n",
    "            ),\n",
    "        ],\n",
    "        job_arguments=[\n",
    "            \"--athena-data\",\n",
    "            athena_data_path,\n",
    "        ],\n",
    "        code=create_dataset_script_path,\n",
    "    )\n",
    "\n",
    "    # baseline job step\n",
    "    # Get the default model monitor container\n",
    "    model_monitor_container_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"model-monitor\",\n",
    "        region=region,\n",
    "        version=\"latest\",\n",
    "    )\n",
    "\n",
    "    # Create the baseline job using\n",
    "    dataset_format = DatasetFormat.csv()\n",
    "    env = {\n",
    "        \"dataset_format\": json.dumps(dataset_format),\n",
    "        \"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\",\n",
    "        \"output_path\": \"/opt/ml/processing/output\",\n",
    "        \"publish_cloudwatch_metrics\": \"Disabled\",\n",
    "    }\n",
    "\n",
    "    monitor_analyzer = Processor(\n",
    "        image_uri=model_monitor_container_uri,\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=baseline_instance_type,\n",
    "        base_job_name=f\"{prefix}/monitoring\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        max_runtime_in_seconds=1800,\n",
    "        env=env,\n",
    "    )\n",
    "\n",
    "    baseline_step = ProcessingStep(\n",
    "        name=\"BaselineJob\",\n",
    "        processor=monitor_analyzer,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"baseline\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/input/baseline_dataset_input\",\n",
    "                input_name=\"baseline_dataset_input\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                source=\"/opt/ml/processing/output\",\n",
    "                output_name=\"monitoring_output\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Model training step\n",
    "    train_instance_count = 1\n",
    "    training_job_output_path = f\"s3://{bucket}/{prefix}/training_jobs\"\n",
    "    metric_uri = f\"{prefix}/training_jobs/metrics_output/metrics.json\"\n",
    "\n",
    "    hyperparameters = {\n",
    "        \"max_depth\": \"3\",\n",
    "        \"eta\": \"0.2\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"num_round\": \"100\",\n",
    "        \"bucket\": f\"{bucket}\",\n",
    "        \"object\": f\"{metric_uri}\",\n",
    "    }\n",
    "\n",
    "    xgb_estimator = XGBoost(\n",
    "        entry_point=model_entry_point,\n",
    "        hyperparameters=hyperparameters,\n",
    "        role=role,\n",
    "        instance_count=train_instance_count,\n",
    "        instance_type=train_instance_param,\n",
    "        framework_version=\"1.0-1\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "    train_step = TrainingStep(\n",
    "        name=\"XgboostTrain\",\n",
    "        estimator=xgb_estimator,\n",
    "        inputs={\n",
    "            \"train\": sagemaker.inputs.TrainingInput(\n",
    "                s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train_data\"\n",
    "                ].S3Output.S3Uri\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # instantiate the Clarify processor\n",
    "    clarify_processor = clarify.SageMakerClarifyProcessor(role=role,\n",
    "                                                      instance_count=1,\n",
    "                                                      instance_type=\"ml.c5.xlarge\",\n",
    "                                                      sagemaker_session=sagemaker_session)\n",
    "    \n",
    "    # Run bias metrics with clarify steps\n",
    "    pipeline_bias_output_path = f\"s3://{bucket}/{prefix}/clarify-output/pipeline/bias\"\n",
    "    \n",
    "    # clarify configuration\n",
    "    bias_data_config = clarify.DataConfig(\n",
    "        s3_data_input_path=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "            \"train_data\"\n",
    "        ].S3Output.S3Uri,\n",
    "        s3_output_path=pipeline_bias_output_path,\n",
    "        label=\"fraud\",\n",
    "        dataset_type=\"text/csv\",\n",
    "    )\n",
    "\n",
    "    bias_config = clarify.BiasConfig(\n",
    "        label_values_or_threshold=[0],\n",
    "        facet_name=\"customer_gender_female\",\n",
    "        facet_values_or_threshold=[1],\n",
    "    )\n",
    "\n",
    "    analysis_config = bias_data_config.get_config()\n",
    "    analysis_config.update(bias_config.get_config())\n",
    "    analysis_config[\"methods\"] = {\"pre_training_bias\": {\"methods\": \"all\"}}\n",
    "\n",
    "    clarify_config_dir = Path(\"config\")\n",
    "    clarify_config_dir.mkdir(exist_ok=True)\n",
    "    with open(clarify_config_dir / \"analysis_config.json\", \"w\") as f:\n",
    "        json.dump(analysis_config, f)  \n",
    "    \n",
    "\n",
    "    clarify_step = ProcessingStep(\n",
    "        name=\"ClarifyProcessor\",\n",
    "        processor=clarify_processor,\n",
    "        inputs=[\n",
    "            sagemaker.processing.ProcessingInput(\n",
    "                input_name=\"analysis_config\",\n",
    "                source=f\"{clarify_config_dir}/analysis_config.json\",\n",
    "                destination=\"/opt/ml/processing/input/config\",\n",
    "            ),\n",
    "            sagemaker.processing.ProcessingInput(\n",
    "                input_name=\"dataset\",\n",
    "                source=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train_data\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/input/data\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            sagemaker.processing.ProcessingOutput(\n",
    "                source=\"/opt/ml/processing/output/analysis.json\",\n",
    "                destination=pipeline_bias_output_path,\n",
    "                output_name=\"analysis_result\",\n",
    "            )\n",
    "        ],\n",
    "    )    \n",
    "    \n",
    "    # Register Model step\n",
    "\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=f\"s3://{bucket}/{metric_uri}\",\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    register_step = RegisterModel(\n",
    "        name=\"RegisterModel\",\n",
    "        estimator=xgb_estimator,\n",
    "        model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.t2.medium\", \"ml.t2.large\", \"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.xlarge\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        approval_status=model_approval_status,\n",
    "        model_metrics=model_metrics,\n",
    "    )\n",
    "\n",
    "    # pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            baseline_instance_type,\n",
    "            train_instance_param,\n",
    "            model_approval_status,\n",
    "        ],\n",
    "        steps=[\n",
    "            create_dataset_step,\n",
    "            baseline_step,\n",
    "            train_step,\n",
    "            clarify_step,\n",
    "            register_step,\n",
    "        ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_columns = [\n",
    "    \"fraud\",\n",
    "    \"incident_severity\",\n",
    "    \"num_vehicles_involved\",\n",
    "    \"num_injuries\",\n",
    "    \"num_witnesses\",\n",
    "    \"police_report_available\",\n",
    "    \"injury_claim\",\n",
    "    \"vehicle_claim\",\n",
    "    \"total_claim_amount\",\n",
    "    \"incident_month\",\n",
    "    \"incident_day\",\n",
    "    \"incident_dow\",\n",
    "    \"incident_hour\",\n",
    "    \"driver_relationship_self\",\n",
    "    \"driver_relationship_na\",\n",
    "    \"driver_relationship_spouse\",\n",
    "    \"driver_relationship_child\",\n",
    "    \"driver_relationship_other\",\n",
    "    \"incident_type_collision\",\n",
    "    \"incident_type_breakin\",\n",
    "    \"incident_type_theft\",\n",
    "    \"collision_type_front\",\n",
    "    \"collision_type_rear\",\n",
    "    \"collision_type_side\",\n",
    "    \"collision_type_na\",\n",
    "    \"authorities_contacted_police\",\n",
    "    \"authorities_contacted_none\",\n",
    "    \"authorities_contacted_fire\",\n",
    "    \"authorities_contacted_ambulance\",\n",
    "    \"customer_age\",\n",
    "    \"customer_education\",\n",
    "    \"months_as_customer\",\n",
    "    \"policy_deductable\",\n",
    "    \"policy_annual_premium\",\n",
    "    \"policy_liability\",\n",
    "    \"auto_year\",\n",
    "    \"num_claims_past_year\",\n",
    "    \"num_insurers_past_5_years\",\n",
    "    \"customer_gender_male\",\n",
    "    \"customer_gender_female\",\n",
    "    \"policy_state_ca\",\n",
    "    \"policy_state_wa\",\n",
    "    \"policy_state_az\",\n",
    "    \"policy_state_or\",\n",
    "    \"policy_state_nv\",\n",
    "    \"policy_state_id\",\n",
    "]\n",
    "training_configuration = dict(\n",
    "    customers_fg_name = customers_feature_group.name,\n",
    "    claims_fg_name = claims_feature_group.name,\n",
    "    create_dataset_script_path = \"scripts/create_dataset.py\",\n",
    "    prefix = prefix,\n",
    "    model_entry_point = \"scripts/xgboost_starter_script.py\",\n",
    "    model_package_group_name = name_from_base(\"xgboost\"),\n",
    "    training_columns=training_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_pipeline = create_pipeline_xgboost_training(\n",
    "    role=role,\n",
    "    pipeline_name=name_from_base('xgboost-develop'),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    **training_configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_pipeline.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_pipeline.create(role_arn=role, description=\"Traing XGBoost model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_pipeline_execution = model_training_pipeline.start(execution_display_name=\"first-build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_pipeline_execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_pipeline_execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto_session.resource('s3')\n",
    "s3_bucket = s3.Bucket(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fg in [claims_feature_group, customers_feature_group]:\n",
    "    s3_prefix = claims_feature_group.describe()['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri'].split('/', 3)[-1]\n",
    "    s3_bucket.delete_objects(\n",
    "        Delete={\n",
    "            'Objects': [\n",
    "                {\n",
    "                    'Key': s3_prefix\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete feature groups\n",
    "claims_feature_group.delete()\n",
    "customers_feature_group.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_pipeline(PipelineName=\"claims-pipeline\")\n",
    "sagemaker_client.delete_pipeline(PipelineName=\"customers-pipeline\")\n",
    "sagemaker_client.delete_pipeline(PipelineName=model_training_pipeline.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
